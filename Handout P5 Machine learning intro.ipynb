{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# P5 : Introduction to Machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary : \n",
    "\n",
    "This is the report made for Udacity's NanoDegree Introduction to Machine Lesson.\n",
    "\n",
    "Subject treatead : \n",
    "\n",
    "- Naive Bayes\n",
    "- SVM\n",
    "- Decision Trees\n",
    "- Choose Your Own Algorithm\n",
    "- Datasets and Questions\n",
    "- Regressions\n",
    "- Outliers\n",
    "- Clustering\n",
    "- Feature Scaling\n",
    "- Text Learning\n",
    "- Feature Selection\n",
    "- PCA\n",
    "- Validation\n",
    "- Evaluation Metrics\n",
    "- Tying It All Together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Link to the [rubric](https://review.udacity.com/#!/rubrics/27/view) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1\n",
    "#### Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. As part of your answer, give some background on the dataset and how it can be used to answer the project question. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this project is to transform data into information, based on a machine learning process. In a less generic description, we plan to achieve the goal by building an algorithm to identify Enron Employees who may have committed fraud based on the public Enron financial and email dataset.\n",
    "\n",
    "Machine learning (ML) is usefull for its suppervised learning techniques. We will build a classifier based on the data we have. \n",
    "\n",
    "The origin of the data is the famous Enron corpus[[0](https://en.wikipedia.org/wiki/Enron_Corpus)] declared of public interest by american Justice in 2002 during Enron's scandal investigations. \n",
    "\n",
    "However, we are given a pre-processed summary:\n",
    "- based on Enron's employees financial Data pdf (Findlaw's) [[1](http://news.findlaw.com/hdocs/docs/enron/enron61702insiderpay.pdf)]\n",
    "- Udacity's mentors work on email communications (from author,to author etc) \n",
    "\n",
    "This summary is provided in form of a python dictionnary with 146 keys representing 146 Enron employees. Here 5 examples ordered by salary.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/lucas/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import os.path\n",
    "import re\n",
    "import sys\n",
    "import pickle\n",
    "import pprint \n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "sys.path.append(\"../tools/\")\n",
    "\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data\n",
    "\n",
    "### Task 1: Select what features you'll use.\n",
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\".\n",
    "features_list = ['poi','salary',\"total_stock_value\",'expenses','other','total_payments'] # You will need to use more features\n",
    "\n",
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)\n",
    "\n",
    "### Task 2: Remove outliers\n",
    "### Task 3: Create new feature(s)\n",
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict\n",
    "\n",
    "import pandas \n",
    "my_dataset_df = pandas.DataFrame.from_dict(my_dataset,orient='index')\n",
    "my_dataset_df.replace('NaN',np.nan, inplace=True)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>salary</th>\n",
       "      <th>to_messages</th>\n",
       "      <th>deferral_payments</th>\n",
       "      <th>total_payments</th>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <th>bonus</th>\n",
       "      <th>restricted_stock</th>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <th>total_stock_value</th>\n",
       "      <th>...</th>\n",
       "      <th>loan_advances</th>\n",
       "      <th>from_messages</th>\n",
       "      <th>other</th>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <th>poi</th>\n",
       "      <th>director_fees</th>\n",
       "      <th>deferred_income</th>\n",
       "      <th>long_term_incentive</th>\n",
       "      <th>email_address</th>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>TOTAL</th>\n",
       "      <td>26704229.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>32083396.0</td>\n",
       "      <td>309886585.0</td>\n",
       "      <td>311764000.0</td>\n",
       "      <td>97343619.0</td>\n",
       "      <td>130322299.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-7576788.0</td>\n",
       "      <td>434509511.0</td>\n",
       "      <td>...</td>\n",
       "      <td>83925000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42667589.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "      <td>1398517.0</td>\n",
       "      <td>-27992891.0</td>\n",
       "      <td>48521928.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SKILLING JEFFREY K</th>\n",
       "      <td>1111258.0</td>\n",
       "      <td>3627.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8682716.0</td>\n",
       "      <td>19250000.0</td>\n",
       "      <td>5600000.0</td>\n",
       "      <td>6843672.0</td>\n",
       "      <td>2042.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>26093672.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>22122.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1920000.0</td>\n",
       "      <td>jeff.skilling@enron.com</td>\n",
       "      <td>88.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LAY KENNETH L</th>\n",
       "      <td>1072321.0</td>\n",
       "      <td>4273.0</td>\n",
       "      <td>202911.0</td>\n",
       "      <td>103559793.0</td>\n",
       "      <td>34348384.0</td>\n",
       "      <td>7000000.0</td>\n",
       "      <td>14761694.0</td>\n",
       "      <td>2411.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>49110078.0</td>\n",
       "      <td>...</td>\n",
       "      <td>81525000.0</td>\n",
       "      <td>36.0</td>\n",
       "      <td>10359729.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-300000.0</td>\n",
       "      <td>3600000.0</td>\n",
       "      <td>kenneth.lay@enron.com</td>\n",
       "      <td>123.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FREVERT MARK A</th>\n",
       "      <td>1060932.0</td>\n",
       "      <td>3275.0</td>\n",
       "      <td>6426990.0</td>\n",
       "      <td>17252530.0</td>\n",
       "      <td>10433518.0</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>4188667.0</td>\n",
       "      <td>2979.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>14622185.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2000000.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>7427621.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3367011.0</td>\n",
       "      <td>1617011.0</td>\n",
       "      <td>mark.frevert@enron.com</td>\n",
       "      <td>242.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PICKERING MARK R</th>\n",
       "      <td>655037.0</td>\n",
       "      <td>898.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1386690.0</td>\n",
       "      <td>28798.0</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>728.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>28798.0</td>\n",
       "      <td>...</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>67.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>mark.pickering@enron.com</td>\n",
       "      <td>7.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        salary  to_messages  deferral_payments  \\\n",
       "TOTAL               26704229.0          NaN         32083396.0   \n",
       "SKILLING JEFFREY K   1111258.0       3627.0                NaN   \n",
       "LAY KENNETH L        1072321.0       4273.0           202911.0   \n",
       "FREVERT MARK A       1060932.0       3275.0          6426990.0   \n",
       "PICKERING MARK R      655037.0        898.0                NaN   \n",
       "\n",
       "                    total_payments  exercised_stock_options       bonus  \\\n",
       "TOTAL                  309886585.0              311764000.0  97343619.0   \n",
       "SKILLING JEFFREY K       8682716.0               19250000.0   5600000.0   \n",
       "LAY KENNETH L          103559793.0               34348384.0   7000000.0   \n",
       "FREVERT MARK A          17252530.0               10433518.0   2000000.0   \n",
       "PICKERING MARK R         1386690.0                  28798.0    300000.0   \n",
       "\n",
       "                    restricted_stock  shared_receipt_with_poi  \\\n",
       "TOTAL                    130322299.0                      NaN   \n",
       "SKILLING JEFFREY K         6843672.0                   2042.0   \n",
       "LAY KENNETH L             14761694.0                   2411.0   \n",
       "FREVERT MARK A             4188667.0                   2979.0   \n",
       "PICKERING MARK R                 NaN                    728.0   \n",
       "\n",
       "                    restricted_stock_deferred  total_stock_value  \\\n",
       "TOTAL                              -7576788.0        434509511.0   \n",
       "SKILLING JEFFREY K                        NaN         26093672.0   \n",
       "LAY KENNETH L                             NaN         49110078.0   \n",
       "FREVERT MARK A                            NaN         14622185.0   \n",
       "PICKERING MARK R                          NaN            28798.0   \n",
       "\n",
       "                             ...            loan_advances  from_messages  \\\n",
       "TOTAL                        ...               83925000.0            NaN   \n",
       "SKILLING JEFFREY K           ...                      NaN          108.0   \n",
       "LAY KENNETH L                ...               81525000.0           36.0   \n",
       "FREVERT MARK A               ...                2000000.0           21.0   \n",
       "PICKERING MARK R             ...                 400000.0           67.0   \n",
       "\n",
       "                         other  from_this_person_to_poi    poi director_fees  \\\n",
       "TOTAL               42667589.0                      NaN  False     1398517.0   \n",
       "SKILLING JEFFREY K     22122.0                     30.0   True           NaN   \n",
       "LAY KENNETH L       10359729.0                     16.0   True           NaN   \n",
       "FREVERT MARK A       7427621.0                      6.0  False           NaN   \n",
       "PICKERING MARK R           NaN                      0.0  False           NaN   \n",
       "\n",
       "                    deferred_income  long_term_incentive  \\\n",
       "TOTAL                   -27992891.0           48521928.0   \n",
       "SKILLING JEFFREY K              NaN            1920000.0   \n",
       "LAY KENNETH L             -300000.0            3600000.0   \n",
       "FREVERT MARK A           -3367011.0            1617011.0   \n",
       "PICKERING MARK R                NaN                  NaN   \n",
       "\n",
       "                               email_address from_poi_to_this_person  \n",
       "TOTAL                                    NaN                     NaN  \n",
       "SKILLING JEFFREY K   jeff.skilling@enron.com                    88.0  \n",
       "LAY KENNETH L          kenneth.lay@enron.com                   123.0  \n",
       "FREVERT MARK A        mark.frevert@enron.com                   242.0  \n",
       "PICKERING MARK R    mark.pickering@enron.com                     7.0  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_dataset_df.sort_values('salary', axis=0, ascending=0, inplace=0).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "One outlier was identified ('TOTAL') and removed with dict.pop method. \n",
    "\n",
    "It's important to notice the low representation of poi : 18/146 ≈ 12%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 \n",
    "\n",
    "#### What features did you end up using in your POI identifier, and what selection process did you use to pick them? Did you have to do any scaling? Why or why not? As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “properly scale features”, “intelligently select feature”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final list of features : ['poi', 'salary', 'total_stock_value', 'deferred_income', 'exercised_stock_options', 'bonus'] \n",
    "\n",
    "Feature selection is by far the most back and forth process of the project. \n",
    "1. At first, we had the intuition that we needed to remove any data points withe some \"missing data\" so to feed the classifier only with clean data.\n",
    "    1. We constructed this matrix of \"feature completness\" based on POI/NO_POI and selected all feature with completness >.85 for POI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": true
   },
   "outputs": [],
   "source": [
    "import collections\n",
    "import pprint\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "poi_df = my_dataset_df.loc[my_dataset_df['poi'] == 1]\n",
    "non_poi_df = my_dataset_df.loc[my_dataset_df['poi'] == 0]\n",
    "pct_dict = collections.defaultdict(dict)\n",
    "for column in my_dataset_df.columns.values:\n",
    "    pct_non_nan_total =  round(my_dataset_df[column].count()/float(len(my_dataset_df[column])),3)\n",
    "    pct_non_nan_poi = poi_df[column].count()/float(len(poi_df[column]))\n",
    "    pct_non_nan_non_poi = non_poi_df[column].count()/float(len(non_poi_df[column]))\n",
    "    \n",
    "    dict_for_column = {\"pct_non_nan_total\":pct_non_nan_total,\n",
    "                      \"pct_non_nan_poi\":pct_non_nan_poi,\n",
    "                      \"pct_non_nan_non_poi\":pct_non_nan_non_poi}\n",
    "    \n",
    "    pct_dict[column]= dict_for_column\n",
    "\n",
    "pct_df = pandas.DataFrame.from_dict(pct_dict,orient='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "hideCode": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pct_non_nan_total</th>\n",
       "      <th>pct_non_nan_poi</th>\n",
       "      <th>pct_non_nan_non_poi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>total_stock_value</th>\n",
       "      <td>0.863</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.843750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>other</th>\n",
       "      <td>0.637</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.585938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total_payments</th>\n",
       "      <td>0.856</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.835938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>email_address</th>\n",
       "      <td>0.760</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>expenses</th>\n",
       "      <td>0.651</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.601562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>poi</th>\n",
       "      <td>1.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>salary</th>\n",
       "      <td>0.651</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.609375</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock</th>\n",
       "      <td>0.753</td>\n",
       "      <td>0.944444</td>\n",
       "      <td>0.726562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bonus</th>\n",
       "      <td>0.562</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.515625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_this_person_to_poi</th>\n",
       "      <td>0.589</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_poi_to_this_person</th>\n",
       "      <td>0.589</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>from_messages</th>\n",
       "      <td>0.589</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>shared_receipt_with_poi</th>\n",
       "      <td>0.589</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>to_messages</th>\n",
       "      <td>0.589</td>\n",
       "      <td>0.777778</td>\n",
       "      <td>0.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>long_term_incentive</th>\n",
       "      <td>0.452</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.421875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>exercised_stock_options</th>\n",
       "      <td>0.699</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.703125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferred_income</th>\n",
       "      <td>0.336</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.296875</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>deferral_payments</th>\n",
       "      <td>0.267</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.265625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loan_advances</th>\n",
       "      <td>0.027</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.023438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>restricted_stock_deferred</th>\n",
       "      <td>0.123</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.140625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>director_fees</th>\n",
       "      <td>0.116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132812</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           pct_non_nan_total  pct_non_nan_poi  \\\n",
       "total_stock_value                      0.863         1.000000   \n",
       "other                                  0.637         1.000000   \n",
       "total_payments                         0.856         1.000000   \n",
       "email_address                          0.760         1.000000   \n",
       "expenses                               0.651         1.000000   \n",
       "poi                                    1.000         1.000000   \n",
       "salary                                 0.651         0.944444   \n",
       "restricted_stock                       0.753         0.944444   \n",
       "bonus                                  0.562         0.888889   \n",
       "from_this_person_to_poi                0.589         0.777778   \n",
       "from_poi_to_this_person                0.589         0.777778   \n",
       "from_messages                          0.589         0.777778   \n",
       "shared_receipt_with_poi                0.589         0.777778   \n",
       "to_messages                            0.589         0.777778   \n",
       "long_term_incentive                    0.452         0.666667   \n",
       "exercised_stock_options                0.699         0.666667   \n",
       "deferred_income                        0.336         0.611111   \n",
       "deferral_payments                      0.267         0.277778   \n",
       "loan_advances                          0.027         0.055556   \n",
       "restricted_stock_deferred              0.123         0.000000   \n",
       "director_fees                          0.116         0.000000   \n",
       "\n",
       "                           pct_non_nan_non_poi  \n",
       "total_stock_value                     0.843750  \n",
       "other                                 0.585938  \n",
       "total_payments                        0.835938  \n",
       "email_address                         0.726562  \n",
       "expenses                              0.601562  \n",
       "poi                                   1.000000  \n",
       "salary                                0.609375  \n",
       "restricted_stock                      0.726562  \n",
       "bonus                                 0.515625  \n",
       "from_this_person_to_poi               0.562500  \n",
       "from_poi_to_this_person               0.562500  \n",
       "from_messages                         0.562500  \n",
       "shared_receipt_with_poi               0.562500  \n",
       "to_messages                           0.562500  \n",
       "long_term_incentive                   0.421875  \n",
       "exercised_stock_options               0.703125  \n",
       "deferred_income                       0.296875  \n",
       "deferral_payments                     0.265625  \n",
       "loan_advances                         0.023438  \n",
       "restricted_stock_deferred             0.140625  \n",
       "director_fees                         0.132812  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pct_df[['pct_non_nan_total','pct_non_nan_poi','pct_non_nan_non_poi']].sort_values('pct_non_nan_poi',ascending=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. With the first 8 features (email_address removed for low information presumptions) most (poor optimized) classifiers where not giving the satisfactory output : Precision,recall >.3\n",
    "3. Classifiers with default parameters started to show better perfomances with datapoints containing missing values\n",
    "4. Ensemble classifiers with default parameters started to show better perfomances with datapoints containing missing values, so we fed the classifier with all the features (except from_this_person_to_poi & from_poi_to_this_person because those features where surrounded by train/test leaked data discussions) \n",
    "5. Ensemble classifier showed better performances with feature selection method SelectKBest[3], with k=10 (tested from k=6 to k=14)\n",
    "6. Ensemble classifier showed worst performances with GridSerchCV parameter fine tunning\n",
    "7. Gaussian Naive Bayes showed very promissing results with the same set of features from SelectKBest, with k=10\n",
    "8. Gaussian Naive Bayes showed improvement in performance with SelectKBest, k=5\n",
    "9. We used FeatureUnion[2] to combine features obtained by PCA and univariate selection with slightly worst performnance\n",
    "10. Definitive feature list : ['poi', 'salary', 'total_stock_value', 'deferred_income', 'exercised_stock_options', 'bonus'] selected by SelectKbest \n",
    "\n",
    "Note on feature creation: After observing similar performances on GNB + SelectkBest with K=3 and K=5 we created a pipeline PCA > SelectKbest\n",
    "\n",
    "```\n",
    "pipeline = Pipeline([(\"features\", combined_features), (\"gnb\", GNB)])\n",
    "param_grid = dict(features__pca__n_components=[1, 2],\n",
    "                   features__univ_select__k=[2,3,4,5,6,7],\n",
    "                   )\n",
    "```\n",
    "\n",
    "The objective was to try to reduce 2 or more feature into the principal component. However, this approach did not bring significant improvement in the metrics evaluated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "hideOutput": true,
    "hidePrompt": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'selector' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-528abb89099c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mprint\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscores_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'selector' is not defined"
     ]
    }
   ],
   "source": [
    "print sorted(selector.scores_, reverse=True)[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": true,
    "hideOutput": false,
    "hidePrompt": true
   },
   "source": [
    "[25.097541528735491, 24.467654047526398, 21.060001707536571, 18.575703268041785, 11.595547659730601]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3\n",
    "#### What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We end up using a default Gaussian Naive Bayes classifier. \n",
    "\n",
    "We tested in this order:\n",
    "\n",
    "1. Gaussian Naive Bayes with remove_any_zeros = True [Precision: 0.36102\tRecall: 0.21300\tF1: 0.26792]\n",
    "2. Decision tree + GridSearchCV [Precision: 0.29597\tRecall: 0.27200\tF1: 0.28348]\n",
    "3. SVC [Error]\n",
    "4. Adabooust default [Precision: 0.34165\tRecall: 0.24650\tF1: 0.28638]\n",
    "5. Adaboost + GridSearchCV [Precision: 0.20610\tRecall: 0.65250\tF1: 0.31325]\n",
    "6. Random forest default [Precision: 0.47015\tRecall: 0.18900\tF1: 0.26961]\n",
    "7. Random forest + GridSearchCV [Precision: 0.44704\tRecall: 0.26800\tF1: 0.33510]\n",
    "6. Gaussian Naive Bayes + SelectkBest k=5 (Precision: 0.48876\tRecall: 0.38050\tF1: 0.42789)\n",
    "\n",
    "Overall our initial strategy with remove_any_zeros = True was proven incorrect. Ensemble classifier did improve the performance at a computational cost (10X slower for fitting and testing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4\n",
    "\n",
    "#### What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? \n",
    "\n",
    "Parameter tunning was done systematically with help of GridSearchCV. It's important because default classifier tend to respond better to a specific dataset once fine tunning is established. In this example , we fine tune by 'n_estimators',learning_rate and algorithm the adaboost classifier. \n",
    "```\n",
    "param_grid = {'n_estimators': [10,45,50,55,60,100],\\\n",
    " 'learning_rate':[1.,2.,5.],\\\n",
    " 'algorithm': ['SAMME', 'SAMME.R']}\n",
    "sss = StratifiedShuffleSplit()\n",
    "gs = GridSearchCV(AdaBoostClassifier(),param_grid,scoring=\"f1\", cv=sss)\n",
    "gs.fit(features,labels)\n",
    "clf = gs.best_estimator_\n",
    "```\n",
    "However the increment in performance were by any means 'game changers'. Feature selection without pre-made asumptions (remove_any_zeros = True) was far more productive.\n",
    "\n",
    "As GNB has no parameter to tune, we made the tunning based on the K of seleckKbest features to select\n",
    "\n",
    ">3 features, Precision: 0.486\tRecall: 0.351\tF1: 0.408\tF2: 0.372\n",
    "\n",
    ">4 features, Precision: 0.503\tRecall: 0.323\tF1: 0.393\tF2: 0.348\n",
    "\n",
    ">**5 features, Precision: 0.489\tRecall: 0.381\tF1: 0.428\tF2: 0.398**\n",
    "\n",
    ">6 features, Precision: 0.457\tRecall: 0.370\tF1: 0.409\tF2: 0.385\n",
    "\n",
    ">7 features, Precision: 0.457\tRecall: 0.384\tF1: 0.417\tF2: 0.397\n",
    "\n",
    ">8 features, Precision: 0.404\tRecall: 0.318\tF1: 0.356\tF2: 0.332\n",
    "\n",
    ">9 features, Precision: 0.328\tRecall: 0.316\tF1: 0.322\tF2: 0.318\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5\n",
    "#### What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric item: “validation strategy”]\n",
    "\n",
    "Validation is defined by the methods we apply to understand if our model does what it was ment to do. Generally it is done by separating part of the dataset (~10%-25%) into training (for selection of your algorithm and parameter settings) and testing sets (how well the fit model generalizes to new data). \n",
    "\n",
    "One classic mistake to avoid is to validate the model on the training data. Performance probably will be (very) good leading to think that the model is ready for production. But performance should be mesured as 'how well the model adapts' to **new** data, hence the initial separation into training/testing sets. \n",
    "\n",
    "In this particular case, initial dataset seizure is complicated as we have very few POI (only 18). \n",
    "\n",
    "So we relied on GridSearchCV based on StratifiedShuffleSplit() in order to have proper cross validation. Why not the classic train/test ?  If we happen to have so little data that we cannot have a separate test set (as is arguably the case in the project), then perhaps the best we can do is to go without the final test set. We no longer have a two-part split of the data into training-validation-test, but instead have the data in one group and we perform cross-validation on the data as a whole.\n",
    "\n",
    "Then we evaluated the overall performance with tester.py provided by the assignment. Tester.py produce a test based on a 1000 fold StratifiedShuffleSplit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6\n",
    "#### Give at least 2 evaluation metrics and your average performance for each of them.  Explain an interpretation of your metrics that says something human-understandable about your algorithm’s performance. [relevant rubric item: “usage of evaluation metrics”]\n",
    "\n",
    "Performance metrics to optimize were Precision and Recall[4]. Accuracy was much less important because model \"All predictions are NON POI\" would have an 83% accuracy. Not inspring confidence. \n",
    "\n",
    "with \n",
    "\n",
    "\n",
    "> true positive (TP)\n",
    "eqv. with hit\n",
    "\n",
    ">true negative (TN)\n",
    "eqv. with correct rejection\n",
    "\n",
    ">false positive (FP)\n",
    "eqv. with false alarm, Type I error\n",
    "\n",
    ">false negative (FN)\n",
    "eqv. with miss, Type II error\n",
    "\n",
    "Precision : TP / (TP + FP) \n",
    "\n",
    "> the fraction of retrieved documents that are relevant to the query\n",
    "\n",
    "Recall : TP / (TP + FN)\n",
    "\n",
    "> the fraction of the documents that are relevant to the query that are successfully retrieved.\n",
    "\n",
    "As explained before our best results were\n",
    "\n",
    ">  **Precision: 0.489 Recall: 0.381 F1: 0.428 F2: 0.398** \n",
    "\n",
    "on a default Gaussian Naive Bayes with features selected with selectKbest [see details in section 4]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hideOutput": false
   },
   "source": [
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [0] https://en.wikipedia.org/wiki/Enron_Corpus\n",
    "- [1] http://news.findlaw.com/hdocs/docs/enron/enron61702insiderpay.pdf\n",
    "- [2] http://scikit-learn.org/stable/auto_examples/feature_stacker.html#sphx-glr-auto-examples-feature-stacker-py\n",
    "- [3] http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html\n",
    "- [4] https://en.wikipedia.org/wiki/Precision_and_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
